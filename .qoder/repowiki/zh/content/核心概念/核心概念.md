# 核心概念

<cite>
**本文档中引用的文件**  
- [base.py](file://libs/core/langchain_core/runnables/base.py)
- [base.py](file://libs/core/langchain_core/callbacks/base.py)
- [base.py](file://libs/core/langchain_core/tracers/base.py)
- [base.py](file://libs/core/langchain_core/prompts/base.py)
- [chat_models.py](file://libs/core/langchain_core/language_models/chat_models.py)
- [llms.py](file://libs/core/langchain_core/language_models/llms.py)
- [config.py](file://libs/core/langchain_core/runnables/config.py)
- [chat.py](file://libs/core/langchain_core/prompts/chat.py)
- [manager.py](file://libs/core/langchain_core/callbacks/manager.py)
- [schemas.py](file://libs/core/langchain_core/tracers/schemas.py)
</cite>

## 目录
1. [简介](#简介)
2. [Runnables抽象](#runnables抽象)
3. [Callbacks和Tracers系统](#callbacks和tracers系统)
4. [Prompt模板系统](#prompt模板系统)
5. [核心模型接口](#核心模型接口)
6. [概念协同工作](#概念协同工作)

## 简介
LangChain框架的核心概念围绕着可组合、可扩展的组件构建，这些组件共同构成了一个强大的语言模型应用开发平台。本文档深入探讨了框架的四个核心支柱：Runnables抽象、Callbacks和Tracers系统、Prompt模板系统以及核心模型接口。这些概念共同支持链式调用、异步执行、监控调试和模型互操作性，为构建复杂的LLM应用程序提供了坚实的基础。

## Runnables抽象
Runnables是LangChain表达式语言（LCEL）的基础，它定义了一个可执行、可批处理、可流式传输、可转换和可组合的工作单元。所有LangChain组件都实现了`Runnable`接口，这使得它们可以无缝地组合在一起。

### 核心方法
`Runnable`接口定义了几个关键方法，支持同步和异步操作：
- **`invoke`/`ainvoke`**: 将单个输入转换为输出。
- **`batch`/`abatch`**: 高效地将多个输入批量转换为输出。
- **`stream`/`astream`**: 在输出生成时流式传输单个输入的输出。
- **`astream_log`**: 流式传输输出和选定的中间结果。

这些方法都接受一个可选的`config`参数，用于配置执行、添加标签和元数据以进行跟踪和调试等。

### 组合与链式调用
Runnables通过LangChain表达式语言（LCEL）进行组合，自动获得同步、异步、批处理和流式支持。主要的组合原语是`RunnableSequence`和`RunnableParallel`。

`RunnableSequence`按顺序调用一系列Runnables，前一个Runnable的输出作为下一个的输入。可以通过`|`操作符或向`RunnableSequence`传递Runnables列表来构建。

`RunnableParallel`并发调用Runnables，为每个提供相同的输入。可以通过序列中的字典字面量或向`RunnableParallel`传递字典来构建。

```python
from langchain_core.runnables import RunnableLambda

# 使用`|`操作符构建的RunnableSequence
sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)
sequence.invoke(1)  # 4
sequence.batch([1, 2, 3])  # [4, 6, 8]

# 包含使用字典字面量构建的RunnableParallel的序列
sequence = RunnableLambda(lambda x: x + 1) | {
    "mul_2": RunnableLambda(lambda x: x * 2),
    "mul_5": RunnableLambda(lambda x: x * 5),
}
sequence.invoke(1)  # {'mul_2': 4, 'mul_5': 10}
```

**Section sources**
- [base.py](file://libs/core/langchain_core/runnables/base.py#L1-L100)

## Callbacks和Tracers系统
Callbacks和Tracers系统是LangChain中用于监控、调试和记录LLM应用程序执行流程的核心机制。它们提供了一种在运行时注入自定义逻辑的方式，以响应各种事件。

### Callbacks基础
`BaseCallbackHandler`是所有回调处理器的基类，它通过多个Mixin类定义了处理不同事件的方法，包括：
- **LLMManagerMixin**: 处理LLM相关事件，如`on_llm_start`、`on_llm_new_token`和`on_llm_end`。
- **ChainManagerMixin**: 处理链相关事件，如`on_chain_start`和`on_chain_end`。
- **ToolManagerMixin**: 处理工具相关事件，如`on_tool_start`和`on_tool_end`。
- **RetrieverManagerMixin**: 处理检索器相关事件，如`on_retriever_start`和`on_retriever_end`。

`AsyncCallbackHandler`提供了异步版本的回调方法，允许在异步执行环境中进行非阻塞的监控和处理。

### Tracers系统
`BaseTracer`是所有跟踪器的基类，它继承自`BaseCallbackHandler`并实现了`_persist_run`抽象方法。Tracers在运行开始时调用`_start_trace`，在运行结束时调用`_end_trace`，并最终通过`_persist_run`将运行数据持久化。

Tracers系统提供了详细的运行时洞察，可以用于：
- **调试**: 通过`set_debug(True)`启用全局调试标志，查看所有链的详细输出。
- **监控**: 通过传递自定义回调处理器（如`ConsoleCallbackHandler`）来监控执行流程。
- **性能分析**: 跟踪每个组件的执行时间和资源消耗。

```python
from langchain_core.globals import set_debug
from langchain_core.tracers import ConsoleCallbackHandler

# 启用全局调试
set_debug(True)

# 或者为特定链传递回调处理器
chain.invoke(..., config={"callbacks": [ConsoleCallbackHandler()]})
```

**Section sources**
- [base.py](file://libs/core/langchain_core/callbacks/base.py#L1-L200)
- [base.py](file://libs/core/langchain_core/tracers/base.py#L1-L150)
- [schemas.py](file://libs/core/langchain_core/tracers/schemas.py#L1-L15)

## Prompt模板系统
Prompt模板系统负责构建和格式化发送给LLM的输入。它提供了一种灵活的方式来创建动态提示，支持变量替换、部分填充和多种输出解析器。

### BasePromptTemplate
`BasePromptTemplate`是所有提示模板的基类，它定义了创建提示所需的核心属性和方法：
- **`input_variables`**: 提示模板所需的变量名称列表。
- **`partial_variables`**: 部分变量的字典，这些变量在模板创建时填充，无需在每次调用时提供。
- **`output_parser`**: 用于解析LLM输出的解析器。

`BasePromptTemplate`实现了`Runnable`接口，因此可以像其他Runnables一样被调用、组合和流式传输。

### 格式化与保存
`BasePromptTemplate`提供了`format`和`format_prompt`方法来格式化提示。`format`方法返回一个格式化的字符串，而`format_prompt`返回一个`PromptValue`对象，该对象可以是`StringPromptValue`或`ChatPromptValueConcrete`。

提示模板可以保存为JSON或YAML文件，便于版本控制和共享。

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("Tell me a joke about {topic}")
# 部分填充
partial_prompt = prompt.partial(topic="cats")
# 格式化
formatted = prompt.format(topic="dogs")
# 保存
prompt.save("path/to/prompt.yaml")
```

**Section sources**
- [base.py](file://libs/core/langchain_core/prompts/base.py#L1-L300)
- [chat.py](file://libs/core/langchain_core/prompts/chat.py#L1-L200)

## 核心模型接口
LangChain通过`BaseLanguageModel`、`BaseChatModel`和`BaseLLM`等核心接口实现了模型互操作性，允许开发者在不同的语言模型之间无缝切换。

### BaseLanguageModel
`BaseLanguageModel`是所有语言模型的抽象基类，它继承自`RunnableSerializable`并定义了与语言模型交互的核心方法：
- **`generate_prompt`**: 将`PromptValue`列表传递给模型并返回生成结果。
- **`agenerate_prompt`**: `generate_prompt`的异步版本。
- **`get_num_tokens`**: 获取文本中的令牌数量，用于检查输入是否适合模型的上下文窗口。

该类还支持缓存、回调和元数据，为模型调用提供了丰富的配置选项。

### BaseChatModel
`BaseChatModel`是专为聊天模型设计的基类，它继承自`BaseLanguageModel`。它要求实现`_generate`方法来生成聊天结果，并可选择实现`_stream`方法以支持流式传输。

```python
class BaseChatModel(BaseLanguageModel[AIMessage], ABC):
    r"""聊天模型的基类。

    关键强制方法：
        实际调用底层模型的方法。
    """
```

**Section sources**
- [base.py](file://libs/core/langchain_core/language_models/base.py#L1-L200)
- [chat_models.py](file://libs/core/langchain_core/language_models/chat_models.py#L1-L200)
- [llms.py](file://libs/core/langchain_core/language_models/llms.py#L1-L200)

## 概念协同工作
LangChain的核心概念协同工作，形成了一个强大而灵活的框架。Runnables提供了可组合性的基础，使得不同的组件可以像乐高积木一样拼接在一起。Callbacks和Tracers系统提供了透明度和可观测性，使得复杂的执行流程可以被监控和调试。Prompt模板系统确保了输入的正确构建和格式化，而核心模型接口则保证了不同模型之间的互操作性。

这些概念共同支持了高级功能，如链式调用、异步执行和流式传输，使得开发者可以构建响应迅速、可扩展的LLM应用程序。通过将这些核心概念结合使用，LangChain为开发复杂的语言模型应用提供了一个坚实的基础。

**Section sources**
- [base.py](file://libs/core/langchain_core/runnables/base.py#L1-L100)
- [base.py](file://libs/core/langchain_core/callbacks/base.py#L1-L100)
- [base.py](file://libs/core/langchain_core/prompts/base.py#L1-L100)
- [base.py](file://libs/core/langchain_core/language_models/base.py#L1-L100)